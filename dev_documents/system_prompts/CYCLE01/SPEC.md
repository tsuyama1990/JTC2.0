# Cycle 01 Specification: Core Framework & Idea Verification

## 1. Summary

Cycle 01 lays the foundational bedrock for "The JTC 2.0". This cycle focuses on establishing the core infrastructure required for a stateful, multi-agent system and implementing the very first "Gate" of the entrepreneurial journey: **Idea Verification**.

The primary objective is to transition from an empty repository to a functioning CLI/Pyxel application where a user can input a broad domain (e.g., "AI in Healthcare"), and the system uses `Tavily` (search) and `OpenAI` (via LangGraph) to brainstorm 10 distinct, validated business ideas. These ideas are not merely random sentences; they must be structured "Lean Canvas" drafts.

This cycle implements the **Orchestrator Pattern** using `LangGraph`. We will define the global `AgentState` schema, which acts as the single source of truth for the entire application lifecycle. We will also set up the `New Employee Agent` (the user's proxy) with the ability to perform research.

Crucially, this cycle introduces the **Human-in-the-Loop (HITL)** pattern. The system must `interrupt` execution after generating ideas, presenting them to the user via a basic User Interface (UI). The user's selection of "Plan A" effectively "validates" the state, allowing the graph to transition to the next phase (which will be built in Cycle 02).

By the end of this cycle, we will have a running application that demonstrates the integration of LLMs, external search tools, state management, and user interaction, fulfilling the "Gate 1" requirement of the "Science of Entrepreneurship" methodology.

## 2. System Architecture

This section details the file structure and code blueprints required for Cycle 01. We are establishing the "Skeleton" of the application.

### File Structure
**Bold files** are to be created or modified in this cycle.

```ascii
.
├── **.env.example**
├── **.env**
├── pyproject.toml
├── README.md
├── src/
│   ├── **__init__.py**
│   ├── agents/
│   │   ├── **__init__.py**
│   │   ├── **base.py**           # Abstract base class for all agents
│   │   └── **employee.py**       # The 'New Employee' Agent logic
│   ├── core/
│   │   ├── **__init__.py**
│   │   ├── **config.py**         # Pydantic Settings (API Keys)
│   │   ├── **state.py**          # Global State Schema
│   │   └── **graph.py**          # LangGraph Workflow Definition
│   ├── data/
│   │   ├── **__init__.py**
│   │   └── **models.py**         # Domain Models (LeanCanvas, Idea)
│   ├── tools/
│   │   ├── **__init__.py**
│   │   └── **tavily_client.py**  # Wrapper for Tavily Search API
│   └── ui/
│       ├── **__init__.py**
│       └── **console.py**        # Temporary CLI for debugging (Pre-Pyxel)
└── tests/
    ├── **__init__.py**
    ├── **test_config.py**
    ├── **test_state.py**
    └── **test_employee_agent.py**
```

### Core Components Blueprints

#### `src/core/state.py`
Defines the structure of the graph state.
```python
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel
import operator

class LeanCanvas(BaseModel):
    problem: str
    solution: str
    customer_segments: str
    unique_value_prop: str
    # ... other fields

class AgentState(TypedDict):
    # The conversation history for the agents
    messages: Annotated[List[str], operator.add]
    # The 10 generated ideas
    ideas: List[LeanCanvas]
    # The selected idea (Plan A)
    selected_idea: Optional[LeanCanvas]
    # Current loop count for safety
    loop_count: int
```

#### `src/agents/employee.py`
The agent responsible for generating ideas.
```python
from langchain_core.messages import HumanMessage, SystemMessage
from src.core.state import AgentState
from src.tools.tavily_client import perform_market_research

def call_employee_agent(state: AgentState):
    """
    The New Employee Agent node.
    It takes the user's input topic and generates 10 lean canvas ideas.
    """
    # Logic to call LLM with system prompt
    # Logic to parse output into List[LeanCanvas]
    return {"ideas": generated_ideas}
```

#### `src/core/graph.py`
The LangGraph definition.
```python
from langgraph.graph import StateGraph, END
from src.core.state import AgentState
from src.agents.employee import call_employee_agent

workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("employee", call_employee_agent)

# Add edges
workflow.set_entry_point("employee")
workflow.add_edge("employee", END)

# Compile with interrupt
app = workflow.compile(interrupt_before=["employee"]) # Actually interrupt AFTER in real logic
```

## 3. Design Architecture

The design philosophy for Cycle 01 is **Type Safety First**. We use Pydantic to ensure that the "Business Ideas" generated by the LLM are not just unstructured text, but valid objects that can be manipulated programmatically in later cycles.

### Domain Models (`src/data/models.py`)

1.  **`BusinessIdea` Schema**:
    *   This is the atomic unit of Cycle 01.
    *   **Fields**:
        *   `title`: A short, catchy name.
        *   `summary`: A 1-sentence elevator pitch.
        *   `target_customer`: Specific niche (e.g., "Dentists in rural Japan").
        *   `pain_point`: The burning need.
        *   `solution`: The proposed fix.
    *   **Constraints**: All fields are required. `summary` must be < 200 chars.

2.  **`LeanCanvas` Schema**:
    *   A more detailed representation extending `BusinessIdea`.
    *   **Fields**: Adds `revenue_streams`, `cost_structure`, `channels`, `key_metrics`, `unfair_advantage`.
    *   **Validation**: Ensure that `problem` and `solution` are logically connected (though this is hard to validate with code, we can enforce non-empty strings).

### Data Flow & Consumers
*   **Producer**: The `New Employee Agent` (powered by OpenAI GPT-4o) produces instances of `LeanCanvas`.
*   **Consumer (Internal)**: The `UI` (Console/Pyxel) consumes the list of `LeanCanvas` objects to display them to the user.
*   **Consumer (External)**: In later cycles, the `Department Head Agents` will consume the `selected_idea` to critique it.

### Extensibility
We are designing the `AgentState` to be additive. In Cycle 01, we only populate `ideas` and `selected_idea`. In Cycle 02, we will add `critiques`. In Cycle 03, we will add `transcript`. By using `TypedDict` with `total=False` or `Optional` fields in Pydantic, we ensure forward compatibility.

## 4. Implementation Approach

This cycle will be executed in a strict sequence to ensure testability at every step.

### Step 1: Environment & Config
1.  Initialize `uv` project (already done).
2.  Create `.env.example` with placeholders for `OPENAI_API_KEY` and `TAVILY_API_KEY`.
3.  Implement `src/core/config.py` using `pydantic-settings` to load these keys safely.
4.  **Verification**: Run a script to print the loaded config (obfuscated).

### Step 2: Tools & Domain Models
1.  Implement `src/data/models.py` defining `BusinessIdea` and `LeanCanvas`.
2.  Implement `src/tools/tavily_client.py`. This should have a function `search_market_needs(query: str) -> str` that returns a summarized string of search results.
3.  **Verification**: Write a unit test that mocks the Tavily API and checks if the parser works.

### Step 3: The Employee Agent
1.  Implement `src/agents/base.py` (optional, for future polymorphism).
2.  Implement `src/agents/employee.py`.
3.  Create the Prompt Template: "You are an entrepreneurial new employee. Given topic X, generate 10 lean canvases..."
4.  Use LangChain's `.with_structured_output(LeanCanvas)` to force JSON output.
5.  **Verification**: Run a standalone script `python src/agents/employee.py` to see if it generates valid JSON for a test topic.

### Step 4: The Graph
1.  Implement `src/core/state.py`.
2.  Implement `src/core/graph.py`.
3.  Define the `interrupt_after` logic. The graph should run the employee node, then stop.
4.  **Verification**: Use `langgraph-cli` or a script to visualise the graph structure.

### Step 5: The UI & Integration
1.  Implement `src/ui/console.py`.
2.  It should:
    *   Ask user for a topic.
    *   Invoke `app.invoke()`.
    *   Print the 10 ideas formatted nicely.
    *   Ask user to select ID (1-10).
    *   Update the state with `selected_idea`.
    *   Resume the graph (or end it, for Cycle 01).

## 5. Test Strategy

We will employ a "Shift Left" testing strategy, validating components before assembling them.

### Unit Testing Approach (Min 300 words)
Unit tests will focus on the deterministic parts of the system: the data models and the configuration loading.

*   **Config Testing**: We will test `src/core/config.py` to ensure it raises validation errors when API keys are missing. This prevents runtime crashes in production.
*   **Model Validation**: We will test `src/data/models.py` with various inputs.
    *   *Test Case*: Pass a valid dictionary -> Assert it parses to `LeanCanvas`.
    *   *Test Case*: Pass a dictionary missing the `problem` field -> Assert it raises `ValidationError`.
    *   *Test Case*: Pass a `summary` > 500 chars -> Assert it raises a custom validator error (if we implement strict limits).
*   **Tool Testing**: We will test `src/tools/tavily_client.py` using `unittest.mock`. We will mock the HTTP response from Tavily to ensure our internal logic handles empty results or API errors gracefully without crashing the agent.

### Integration Testing Approach (Min 300 words)
Integration tests will verify the interaction between the Agent, the LLM (mocked or real), and the LangGraph state machine.

*   **Agent-LLM Integration**: We will write a test that calls `call_employee_agent` with a `MagicMock` LLM. We will verify that the agent correctly formats the prompt sent to the LLM and correctly parses the mocked JSON response into the `AgentState`.
*   **Graph State Transitions**: We will test the workflow in `src/core/graph.py`.
    *   *Scenario*: Start graph -> Agent Node -> Interrupt.
    *   We will verify that after the Agent Node runs, the state contains 10 ideas.
    *   We will verify that the graph execution status is `suspended` (waiting for input).
*   **Live Smoke Test**: A manual (or scripted) execution where we actually call OpenAI and Tavily (using a dev key) to ensure the prompt engineering is effective. We will check if the generated ideas are actually relevant to the input topic. *Note: These tests will be marked with `@pytest.mark.slow` or `@pytest.mark.integration` to be excluded from standard CI runs.*
